{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T01:38:22.986161Z",
     "iopub.status.busy": "2021-01-01T01:38:22.985317Z",
     "iopub.status.idle": "2021-01-01T01:38:22.988219Z",
     "shell.execute_reply": "2021-01-01T01:38:22.987699Z"
    },
    "papermill": {
     "duration": 0.021668,
     "end_time": "2021-01-01T01:38:22.988325",
     "exception": false,
     "start_time": "2021-01-01T01:38:22.966657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T01:38:23.026949Z",
     "iopub.status.busy": "2021-01-01T01:38:23.025460Z",
     "iopub.status.idle": "2021-01-01T01:38:29.586811Z",
     "shell.execute_reply": "2021-01-01T01:38:29.585769Z"
    },
    "id": "ULLVYzlKus93",
    "papermill": {
     "duration": 6.58631,
     "end_time": "2021-01-01T01:38:29.586931",
     "exception": false,
     "start_time": "2021-01-01T01:38:23.000621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt \n",
    "import tifffile \n",
    "import os\n",
    "import cv2 \n",
    "import tensorflow as tf \n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dense\n",
    "#from keras.layers import Flatten\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D, AveragePooling2D, Conv2DTranspose\n",
    "from keras.layers.merge import concatenate #Concatenate (capital C) not working \n",
    "#from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T01:38:29.639050Z",
     "iopub.status.busy": "2021-01-01T01:38:29.638445Z",
     "iopub.status.idle": "2021-01-01T01:38:29.835187Z",
     "shell.execute_reply": "2021-01-01T01:38:29.835690Z"
    },
    "id": "eEjIAtrjvK2Y",
    "outputId": "d47eaf47-05ac-4e7b-8540-94553bd300e3",
    "papermill": {
     "duration": 0.236176,
     "end_time": "2021-01-01T01:38:29.835818",
     "exception": false,
     "start_time": "2021-01-01T01:38:29.599642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3849\n",
      "3849\n"
     ]
    }
   ],
   "source": [
    "#elastic deformation with rotation and small shift for making the model robustness\n",
    "def Augmentation(images,mask):\n",
    "    def _transform(image,mask):\n",
    "        transform=A.Compose([\n",
    "         A.HorizontalFlip(p=0.25),\n",
    "         A.VerticalFlip(p=0.25),\n",
    "         A.RandomRotate90(p=.25),\n",
    "         A.Transpose(p=0.25),\n",
    "         A.ElasticTransform(p=.2, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "    ])\n",
    "        aug=transform(image=image,mask=mask)\n",
    "        img=aug['image']\n",
    "        img=tf.cast(img,tf.float64)\n",
    "        msk=aug['mask']\n",
    "        msk=tf.cast(msk,tf.float64)\n",
    "        return img,msk\n",
    "    image,mask=tf.numpy_function(_transform,[images,mask],[tf.float64,tf.float64])\n",
    "    image.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "    mask.set_shape([IMAGE_SIZE, IMAGE_SIZE, 1])\n",
    "    \n",
    "    return image,mask\n",
    "from glob import glob\n",
    "IMAGE_SIZE=512\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "def load_data(path, split=0.2):\n",
    "    images = sorted(glob(os.path.join(path, \"train/*\")))\n",
    "    print(len(images))\n",
    "    masks = sorted(glob(os.path.join(path, \"masks/*\")))\n",
    "    print(len(masks))\n",
    "    total_size = len(images)\n",
    "    valid_size = int(split * total_size)\n",
    "\n",
    "    train_x, valid_x = train_test_split(images, test_size=valid_size, random_state=42)\n",
    "    train_y, valid_y = train_test_split(masks, test_size=valid_size, random_state=42)\n",
    "\n",
    "    return (train_x, train_y), (valid_x, valid_y)\n",
    "(train_x,train_y),(valid_x,valid_y)=load_data('../input/hubmap-1024x1024')\n",
    "train_size=len(train_x)\n",
    "valid_size=len(valid_x)\n",
    "def read_image(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x,(512,512),interpolation=cv2.INTER_AREA)\n",
    "    x = x/255.0\n",
    "    x=tf.cast(x,dtype=tf.float64)\n",
    "    return x\n",
    "def read_mask(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x,(512,512),interpolation=cv2.INTER_NEAREST)\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    x=tf.cast(x,dtype=tf.float64)\n",
    "    return x\n",
    "def parser(x,y):\n",
    "    def _parse(x,y):\n",
    "        x=read_image(x)\n",
    "        y=read_mask(y)\n",
    "        return x,y\n",
    "    x,y = tf.numpy_function(_parse, [x,y], [tf.float64,tf.float64])\n",
    "    x.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "    y.set_shape([IMAGE_SIZE, IMAGE_SIZE, 1])\n",
    "    return x,y\n",
    "    \n",
    "def tf_dataset(x, y, batch):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.map(Augmentation)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch)\n",
    "\n",
    "    return dataset\n",
    "#train_dataset=tf_dataset(train_x,train_y,batch=4)\n",
    "#valid_dataset=tf_dataset(valid_x,valid_y,batch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T01:38:29.865382Z",
     "iopub.status.busy": "2021-01-01T01:38:29.864674Z",
     "iopub.status.idle": "2021-01-01T01:38:30.205071Z",
     "shell.execute_reply": "2021-01-01T01:38:30.204589Z"
    },
    "id": "aChgnXblvVXq",
    "outputId": "2c60835c-f6de-4b78-84e2-e3e3923d2832",
    "papermill": {
     "duration": 0.356349,
     "end_time": "2021-01-01T01:38:30.205172",
     "exception": false,
     "start_time": "2021-01-01T01:38:29.848823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-69d851b38e1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T01:38:30.236874Z",
     "iopub.status.busy": "2021-01-01T01:38:30.235110Z",
     "iopub.status.idle": "2021-01-01T01:38:30.237631Z",
     "shell.execute_reply": "2021-01-01T01:38:30.238090Z"
    },
    "id": "ROsmzUlByByy",
    "papermill": {
     "duration": 0.019984,
     "end_time": "2021-01-01T01:38:30.238194",
     "exception": false,
     "start_time": "2021-01-01T01:38:30.218210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_WIDTH=1024\n",
    "IMAGE_HEIGHT=1024\n",
    "IMAGE_CHANNELS=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COusItRPjdDk",
    "papermill": {
     "duration": 0.012951,
     "end_time": "2021-01-01T01:38:30.263875",
     "exception": false,
     "start_time": "2021-01-01T01:38:30.250924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# loss functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T01:38:30.305529Z",
     "iopub.status.busy": "2021-01-01T01:38:30.304676Z",
     "iopub.status.idle": "2021-01-01T01:38:30.307364Z",
     "shell.execute_reply": "2021-01-01T01:38:30.306890Z"
    },
    "id": "x830zCityEpv",
    "papermill": {
     "duration": 0.030692,
     "end_time": "2021-01-01T01:38:30.307457",
     "exception": false,
     "start_time": "2021-01-01T01:38:30.276765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "smooth = 1\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = tf.keras.layers.Flatten()(y_true)\n",
    "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n",
    "\n",
    "def binary_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "def binary_dice_loss(y_true, y_pred):\n",
    "    d_loss= dice_loss(y_true, y_pred)\n",
    "    b_loss= binary_loss(y_true, y_pred)\n",
    "    loss=(d_loss+b_loss)\n",
    "    return loss\n",
    "\n",
    "def Tversky_Index(y_true,y_pred,smooth):\n",
    "    y_true= tf.keras.layers.Flatten()(y_true)\n",
    "    y_pred= tf.keras.layers.Flatten()(y_pred)\n",
    "    true_positive= tf.reduce_sum(y_true*y_pred)\n",
    "    false_negative=tf.reduce_sum(y_true*(1-y_pred))\n",
    "    false_positive=tf.reduce_sum((1-y_true)*y_pred)\n",
    "    alpha=.7\n",
    "    TI=(true_positive+smooth)/(true_positive+alpha*false_negative+(1-alpha)*false_positive+smooth)\n",
    "    return TI\n",
    "\n",
    "def Tversky_loss(y_true,y_pred):\n",
    "    return 1-Tversky_Index(y_true,y_pred,smooth=1)\n",
    "def focal_Tversky_loss(y_true,y_pred):\n",
    "    pt_1 = Tversky_Index(y_true, y_pred,smooth=1)\n",
    "    gamma = 0.75\n",
    "    return tf.math.pow((1-pt_1), gamma)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c92zCOSs0ttg",
    "papermill": {
     "duration": 0.012771,
     "end_time": "2021-01-01T01:38:30.333080",
     "exception": false,
     "start_time": "2021-01-01T01:38:30.320309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T01:38:30.364318Z",
     "iopub.status.busy": "2021-01-01T01:38:30.363564Z",
     "iopub.status.idle": "2021-01-01T01:38:30.366676Z",
     "shell.execute_reply": "2021-01-01T01:38:30.366074Z"
    },
    "id": "aGpPOrZgY2yS",
    "papermill": {
     "duration": 0.020683,
     "end_time": "2021-01-01T01:38:30.366764",
     "exception": false,
     "start_time": "2021-01-01T01:38:30.346081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.applications import VGG19\n",
    "from keras.layers import GlobalAveragePooling2D \n",
    "from keras.layers import multiply, Reshape\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T01:38:30.405373Z",
     "iopub.status.busy": "2021-01-01T01:38:30.400123Z",
     "iopub.status.idle": "2021-01-01T01:38:30.435100Z",
     "shell.execute_reply": "2021-01-01T01:38:30.434611Z"
    },
    "id": "OFeSAXSUEgwg",
    "outputId": "24e56c31-79ae-4f75-f65f-411e05773a5e",
    "papermill": {
     "duration": 0.055619,
     "end_time": "2021-01-01T01:38:30.435185",
     "exception": false,
     "start_time": "2021-01-01T01:38:30.379566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def squeeze_excite_block(inputs, ratio=8):\n",
    "    init = inputs\n",
    "    channel_axis = -1\n",
    "    filters = init.shape[channel_axis]\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    x = multiply([init, se])\n",
    "    return x\n",
    "\n",
    "def conv_block(inputs, filters):\n",
    "    x = inputs\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = squeeze_excite_block(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def encoder1(inputs):\n",
    "    skip_connections = []\n",
    "\n",
    "    model = VGG19(include_top=False, weights='../input/pretrained-vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', input_tensor=inputs)\n",
    "    print(model.summary())\n",
    "    names = [\"block1_conv2\", \"block2_conv2\", \"block3_conv4\", \"block4_conv4\"]\n",
    "    for name in names:\n",
    "        skip_connections.append(model.get_layer(name).output)\n",
    "\n",
    "    output = model.get_layer(\"block5_conv4\").output\n",
    "    return output, skip_connections\n",
    "\n",
    "def decoder1(inputs, skip_connections):\n",
    "    num_filters = [256, 128, 64, 32]\n",
    "    skip_connections.reverse()\n",
    "    x = inputs\n",
    "\n",
    "    for i, f in enumerate(num_filters):\n",
    "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
    "        x = concatenate([x, skip_connections[i]])\n",
    "        x = conv_block(x, f)\n",
    "\n",
    "    return x\n",
    "def encoder2(inputs):\n",
    "    num_filters = [32, 64, 128, 256]\n",
    "    skip_connections = []\n",
    "    x = inputs\n",
    "\n",
    "    for i, f in enumerate(num_filters):\n",
    "        x = conv_block(x, f)\n",
    "        skip_connections.append(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    return x, skip_connections\n",
    "\n",
    "def decoder2(inputs, skip_1, skip_2):\n",
    "    num_filters = [256, 128, 64, 32]\n",
    "    skip_2.reverse()\n",
    "    x = inputs\n",
    "\n",
    "    for i, f in enumerate(num_filters):\n",
    "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
    "        x = concatenate([x, skip_1[i], skip_2[i]])\n",
    "        x = conv_block(x, f)\n",
    "\n",
    "    return x\n",
    "\n",
    "def output_block(inputs):\n",
    "    x = Conv2D(1, (1, 1), padding=\"same\")(inputs)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    return x\n",
    "\n",
    "def Upsample(tensor, size):\n",
    "    \"\"\"Bilinear upsampling\"\"\"\n",
    "    def _upsample(x, size):\n",
    "        return tf.image.resize(images=x, size=size)\n",
    "    return Lambda(lambda x: _upsample(x, size), output_shape=size)(tensor)\n",
    "def ASPP(x, filter):\n",
    "    shape = x.shape\n",
    "\n",
    "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)\n",
    "    y1 = Conv2D(filter, 1, padding=\"same\")(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation(\"relu\")(y1)\n",
    "    y1 = UpSampling2D((shape[1], shape[2]), interpolation='bilinear')(y1)\n",
    "\n",
    "    y2 = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(x)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    y2 = Activation(\"relu\")(y2)\n",
    "\n",
    "    y3 = Conv2D(filter, 3, dilation_rate=6, padding=\"same\", use_bias=False)(x)\n",
    "    y3 = BatchNormalization()(y3)\n",
    "    y3 = Activation(\"relu\")(y3)\n",
    "\n",
    "    y4 = Conv2D(filter, 3, dilation_rate=12, padding=\"same\", use_bias=False)(x)\n",
    "    y4 = BatchNormalization()(y4)\n",
    "    y4 = Activation(\"relu\")(y4)\n",
    "\n",
    "    y5 = Conv2D(filter, 3, dilation_rate=18, padding=\"same\", use_bias=False)(x)\n",
    "    y5 = BatchNormalization()(y5)\n",
    "    y5 = Activation(\"relu\")(y5)\n",
    "\n",
    "    y = concatenate([y1, y2, y3, y4, y5])\n",
    "\n",
    "    y = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "\n",
    "    return y\n",
    "def build_model(shape):\n",
    "    inputs = Input(shape)\n",
    "    x, skip_1 = encoder1(inputs)\n",
    "    x = ASPP(x, 64)\n",
    "    x = decoder1(x, skip_1)\n",
    "    outputs1 = output_block(x)\n",
    "\n",
    "    x = inputs * outputs1\n",
    "\n",
    "    x, skip_2 = encoder2(x)\n",
    "    x = ASPP(x, 64)\n",
    "    x = decoder2(x, skip_1, skip_2)\n",
    "    outputs2 = output_block(x)\n",
    "    #outputs = concatenate([outputs1, outputs2])\n",
    "    #output=Conv2D(1,(2,2),activation=\"sigmoid\",padding=\"same\")(outputs)\n",
    "    model = Model(inputs, outputs2)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T01:38:30.474752Z",
     "iopub.status.busy": "2021-01-01T01:38:30.474015Z",
     "iopub.status.idle": "2021-01-01T01:38:30.476443Z",
     "shell.execute_reply": "2021-01-01T01:38:30.477000Z"
    },
    "papermill": {
     "duration": 0.028998,
     "end_time": "2021-01-01T01:38:30.477101",
     "exception": false,
     "start_time": "2021-01-01T01:38:30.448103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rle_encode_less_memory(img):\n",
    "    pixels = img.T.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "\n",
    "def make_grid(shape, window=256, min_overlap=32):\n",
    "    \"\"\"\n",
    "        Return Array of size (N,4), where N - number of tiles,\n",
    "        2nd axis represente slices: x1,x2,y1,y2 \n",
    "    \"\"\"\n",
    "    x, y = shape\n",
    "    nx = x // (window - min_overlap) + 1\n",
    "    x1 = np.linspace(0, x, num=nx, endpoint=False, dtype=np.int64)\n",
    "    x1[-1] = x - window\n",
    "    x2 = (x1 + window).clip(0, x)\n",
    "    ny = y // (window - min_overlap) + 1\n",
    "    y1 = np.linspace(0, y, num=ny, endpoint=False, dtype=np.int64)\n",
    "    y1[-1] = y - window\n",
    "    y2 = (y1 + window).clip(0, y)\n",
    "    slices = np.zeros((nx,ny, 4), dtype=np.int64)\n",
    "    \n",
    "    for i in range(nx):\n",
    "        for j in range(ny):\n",
    "            slices[i,j] = x1[i], x2[i], y1[j], y2[j]    \n",
    "    return slices.reshape(nx*ny,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T01:38:30.506987Z",
     "iopub.status.busy": "2021-01-01T01:38:30.506402Z",
     "iopub.status.idle": "2021-01-01T01:40:41.720458Z",
     "shell.execute_reply": "2021-01-01T01:40:41.719400Z"
    },
    "papermill": {
     "duration": 131.230711,
     "end_time": "2021-01-01T01:40:41.720594",
     "exception": false,
     "start_time": "2021-01-01T01:38:30.489883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "desnet_model=tf.keras.models.load_model('../input/desnetpretrained6/models4.hd5',custom_objects={\n",
    "        'focal_Tversky_loss': focal_Tversky_loss ,'dice_coef': dice_coef})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T01:40:41.764419Z",
     "iopub.status.busy": "2021-01-01T01:40:41.763719Z",
     "iopub.status.idle": "2021-01-01T02:08:49.935533Z",
     "shell.execute_reply": "2021-01-01T02:08:49.933780Z"
    },
    "id": "cwBAdiFIinhw",
    "papermill": {
     "duration": 1688.20239,
     "end_time": "2021-01-01T02:08:49.936505",
     "exception": false,
     "start_time": "2021-01-01T01:40:41.734115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/rasterio/__init__.py:221: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Predicting afa5e8098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [06:51<27:27, 411.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Predicting b9a3865fc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [12:30<19:29, 390.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Predicting c68fe75ea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [18:50<12:54, 387.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Predicting b2dc8411c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [20:48<05:06, 306.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Predicting 26dc41664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [28:07<00:00, 337.57s/it]\n"
     ]
    }
   ],
   "source": [
    "import rasterio \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm \n",
    "import pathlib\n",
    "import gc\n",
    "from rasterio.windows import Window\n",
    "identity = rasterio.Affine(1, 0, 0, 0, 1, 0)\n",
    "WINDOW,MIN_OVERLAP=1024,300\n",
    "p = pathlib.Path('../input/hubmap-kidney-segmentation')\n",
    "NEW_SIZE=512\n",
    "subm = {}\n",
    "THRESHOLD=0.7\n",
    "for i, filename in tqdm(enumerate(p.glob('test/*.tiff')), \n",
    "                        total = len(list(p.glob('test/*.tiff')))):\n",
    "    \n",
    "    print(f'{i+1} Predicting {filename.stem}')\n",
    "    \n",
    "    dataset = rasterio.open(filename.as_posix(), transform = identity)\n",
    "    slices = make_grid(dataset.shape, window=WINDOW, min_overlap=MIN_OVERLAP)\n",
    "    preds = np.zeros(dataset.shape, dtype=np.uint8)\n",
    "    \n",
    "    for (x1,x2,y1,y2) in slices:\n",
    "        image = dataset.read([1,2,3],\n",
    "                    window=Window.from_slices((x1,x2),(y1,y2)))\n",
    "        image = np.moveaxis(image, 0, -1)\n",
    "        image = cv2.resize(image, (NEW_SIZE, NEW_SIZE),interpolation = cv2.INTER_AREA)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        image = image/255.0\n",
    "        image = np.expand_dims(image, 0)\n",
    "        \n",
    "        pred = None\n",
    "        \n",
    "        \n",
    "        if pred is None:\n",
    "            pred = np.squeeze(desnet_model.predict(image))\n",
    "        else:\n",
    "            pred += np.squeeze(desnet_model.predict(image))\n",
    "        \n",
    "        pred = pred\n",
    "        \n",
    "        pred = cv2.resize(pred, (WINDOW, WINDOW))\n",
    "        preds[x1:x2,y1:y2] += (pred > THRESHOLD).astype(np.uint8)\n",
    "    \n",
    "    preds = (preds > 0.7).astype(np.uint8)\n",
    "    \n",
    "    subm[i] = {'id':filename.stem, 'predicted': rle_encode_less_memory(preds)}\n",
    "    del preds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T02:08:50.003870Z",
     "iopub.status.busy": "2021-01-01T02:08:49.998446Z",
     "iopub.status.idle": "2021-01-01T02:08:51.113172Z",
     "shell.execute_reply": "2021-01-01T02:08:51.112681Z"
    },
    "papermill": {
     "duration": 1.158859,
     "end_time": "2021-01-01T02:08:51.113283",
     "exception": false,
     "start_time": "2021-01-01T02:08:49.954424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afa5e8098</td>\n",
       "      <td>15967613 19 16004411 29 16041210 34 16078009 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b9a3865fc</td>\n",
       "      <td>61146700 28 61177980 46 61209262 65 61240553 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c68fe75ea</td>\n",
       "      <td>21122582 19 21149419 28 21176255 41 21203093 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b2dc8411c</td>\n",
       "      <td>18270166 16 18284999 35 18285402 54 18285459 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26dc41664</td>\n",
       "      <td>24302898 31 24303173 14 24341056 54 24341148 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                          predicted\n",
       "0  afa5e8098  15967613 19 16004411 29 16041210 34 16078009 3...\n",
       "1  b9a3865fc  61146700 28 61177980 46 61209262 65 61240553 7...\n",
       "2  c68fe75ea  21122582 19 21149419 28 21176255 41 21203093 4...\n",
       "3  b2dc8411c  18270166 16 18284999 35 18285402 54 18285459 3...\n",
       "4  26dc41664  24302898 31 24303173 14 24341056 54 24341148 2..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame.from_dict(subm, orient='index')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T02:08:51.155230Z",
     "iopub.status.busy": "2021-01-01T02:08:51.154661Z",
     "iopub.status.idle": "2021-01-01T02:08:51.162871Z",
     "shell.execute_reply": "2021-01-01T02:08:51.162309Z"
    },
    "papermill": {
     "duration": 0.031474,
     "end_time": "2021-01-01T02:08:51.162972",
     "exception": false,
     "start_time": "2021-01-01T02:08:51.131498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tifffile \n",
    "p= pathlib.Path('../input/hubmap-kidney-segmentation')\n",
    "import os \n",
    "from glob import glob \n",
    "path=\"../input/hubmap-kidney-segmentation/test\"\n",
    "data=sorted(glob(os.path.join(path,'test/*.tiff')))\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1834.354313,
   "end_time": "2021-01-01T02:08:52.876439",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-01T01:38:18.522126",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
